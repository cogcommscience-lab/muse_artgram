{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4ed30-4ef5-4cb3-9420-2ee0e0068cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5e623-de03-4cef-9c6b-09db24fb2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After step 1 is complete\n",
    "# Load in the preprocessed and structured eeg data\n",
    "\n",
    "# This code defines file for opening a pickeled object\n",
    "# Update filename based on epoch duration\n",
    "file = open(\"preprocessed_interpolate_bandpass_.1_20/preprocessed_data_1s_epoch.obj\",'rb')\n",
    "\n",
    "\n",
    "# This code reads in the pickle file\n",
    "preprocess_dict = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bf0ea-0e3f-4526-bff5-b7324ccd4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure data as a long dataframe\n",
    "\n",
    "# Initalize empty lists for structuring data\n",
    "eeg_long_df_list = []\n",
    "subj_list = []\n",
    "ses_list = []\n",
    "\n",
    "# For loop that reads in data from preprocess_dict and stores it with the new variables\n",
    "for subj in list(preprocess_dict.keys()):\n",
    "    for ses in list(preprocess_dict[subj].keys()):\n",
    "        temp_df = preprocess_dict[subj][ses]\n",
    "        eeg_long_df_list.append(temp_df)\n",
    "        subj_list.append([subj] * temp_df.shape[0])\n",
    "        ses_list.append([ses] * temp_df.shape[0])\n",
    "        \n",
    "subj_list = [item for sublist in subj_list for item in sublist]\n",
    "ses_list = [item for sublist in ses_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b30a92-d4be-46a2-b9ab-e00b39cccf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a dataframe\n",
    "eeg_long_df = pd.concat(eeg_long_df_list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b437be9-6ac1-4f09-95b5-66a4cd9f315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subj_list and ses_list to eeg_long_df\n",
    "eeg_long_df[\"subj\"] = subj_list\n",
    "eeg_long_df[\"ses\"] = ses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79a98e-d128-4732-9b7a-b601f3f8bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file for later processing \n",
    "# Update file name depending on epoch duration\n",
    "\n",
    "eeg_long_df.to_csv('preprocessed_interpolate_bandpass_.1_20/eeg_long_df_1s_epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127368a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df\n",
    "# This is where you will store the number of cells rejected by autoreject\n",
    "# Filtered by subj by session by electrode\n",
    "zero_counts_df = pd.DataFrame(columns=['subj', 'ses', 'electrode', 'zero_count', 'nonzero_count', 'total', 'percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dda454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define electrode columns that you will filter by in eeg_long_df\n",
    "electrodes = ['AF7-1', 'AF8-1', 'TP9-1', 'TP10-1', 'AF7-2', 'AF8-2', 'TP9-2', 'TP10-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Count zeros and non-zeros for each subj, ses, and electrode\n",
    "# Calculate total number or rows for each subj for each session for each electrode\n",
    "# Calculate the percentage of rows dropped for each subj for each session for each electrode\n",
    "for (subj, ses), group in eeg_long_df.groupby(['subj', 'ses']):\n",
    "    for electrode in electrodes:\n",
    "        zero_count = (group[electrode] == 0).sum()\n",
    "        nonzero_count = (group[electrode] > 0).sum()\n",
    "        total = zero_count + nonzero_count\n",
    "        percent = (zero_count / total) * 100 if total != 0 else 0  # To handle division by zero\n",
    "\n",
    "        results.append({\n",
    "            'subj': subj,\n",
    "            'ses': ses,\n",
    "            'electrode': electrode,\n",
    "            'zero_count': zero_count,\n",
    "            'nonzero_count': nonzero_count,\n",
    "            'total': total,\n",
    "            'percent': percent\n",
    "        })\n",
    "\n",
    "# Convert list of dicts to DataFrame\n",
    "zero_counts_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbe386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average number of data dropped:\n",
    "\n",
    "average_percent = zero_counts_df['percent'].mean()\n",
    "print(\"Average percentage of zero counts:\", average_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b60673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv for subsequent processing\n",
    "# Update file path name as necessary\n",
    "\n",
    "df_average_percent = pd.DataFrame({'Average_Percent': [average_percent]})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_average_percent.to_csv('data_dropped/average_percent_4s_epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffbda8-3a6d-48b0-89f7-acb33ee7cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rejected epochs, autoreject replaces that value with a zero\n",
    "# That creates a problem when trying to calculate ISCs\n",
    "# This is because lots of zeros artifically reduce correlation magnitude\n",
    "# This code replaces zero wit nan to deal with that issue\n",
    "eeg_long_df[[\"TP9-1\", \"AF7-1\", \"AF8-1\", \"TP10-1\", \"TP9-2\", \"AF7-2\", \"AF8-2\", \"TP10-2\"]] = \\\n",
    "eeg_long_df.loc[:, [\"TP9-1\", \"AF7-1\", \"AF8-1\", \"TP10-1\", \"TP9-2\", \"AF7-2\", \"AF8-2\", \"TP10-2\"]].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ffded-998a-4b31-88f5-02971999fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure EEG inter-subject correlation\n",
    "# Make a big correlation matrix for each electrode for each pair of participants\n",
    "# Then index that correlation matrix, selecting the correct cell and restructuring as a dataframe\n",
    "\n",
    "corr_df_TP9 = eeg_long_df.groupby([\"subj\", \"ses\"])[[\"TP9-1\", \"TP9-2\"]].corr().iloc[0::2,-1].reset_index().iloc[:,-1]\n",
    "corr_df_AF7 = eeg_long_df.groupby([\"subj\", \"ses\"])[[\"AF7-1\", \"AF7-2\"]].corr().iloc[0::2,-1].reset_index().iloc[:,-1]\n",
    "corr_df_AF8 = eeg_long_df.groupby([\"subj\", \"ses\"])[[\"AF8-1\", \"AF8-2\"]].corr().iloc[0::2,-1].reset_index().iloc[:,-1]\n",
    "corr_df_TP10 = eeg_long_df.groupby([\"subj\", \"ses\"])[[\"TP10-1\", \"TP10-2\"]].corr().iloc[0::2,-1].reset_index().iloc[:,-1]\n",
    "\n",
    "# Combine individual electrode correlation dataframes into one big dataframe \n",
    "corr_df = pd.concat([corr_df_TP9, corr_df_AF7, corr_df_AF8, corr_df_TP10], axis=1)\n",
    "corr_df.columns = [\"TP9\", \"AF7\", \"AF8\", \"TP10\"]\n",
    "corr_df[\"subj\"] = eeg_long_df.groupby([\"subj\", \"ses\"])[[\"TP9-1\", \"TP9-2\"]].corr().iloc[0::2,-1].reset_index().iloc[:,0]\n",
    "corr_df[\"ses\"] = eeg_long_df.groupby([\"subj\", \"ses\"])[[\"TP9-1\", \"TP9-2\"]].corr().iloc[0::2,-1].reset_index().iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63901c1b-d8f7-4ab4-b5cd-914e67e206f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the structured task performance data\n",
    "\n",
    "task_performance_df = pd.read_csv(\"task_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eac86f-357c-4045-b769-86122b073bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'subj' and 'ses' columns of the corr_df dataframe from their current data types to integers\n",
    "corr_df[\"subj\"] = corr_df[\"subj\"].astype(int)\n",
    "corr_df[\"ses\"] = corr_df[\"ses\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76faf7-a38b-4d7e-abe9-fc52ce13540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'corr_df' dataframe with 'task_performance_df' on columns 'subj' and 'ses', using a left join\n",
    "# This operation adds the columns from 'task_performance_df' to 'corr_df' based on matching 'subj' and 'ses' values\n",
    "# Rows in 'corr_df' that do not match in 'task_performance_df' will be included but have NaN values for the new columns\n",
    "\n",
    "corr_df = corr_df.merge(task_performance_df, on=[\"subj\", \"ses\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af51099-dff2-4b7c-95e5-31024f6fe416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows from the 'corr_df' dataframe that contain any missing values (NaNs)\n",
    "\n",
    "corr_df = corr_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d56f96-d684-4de4-a86d-c61bb1fadd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique number of subjects in the 'subj' column of the 'corr_df' dataframe\n",
    "# This is achieved by selecting the 'subj' column, calling .unique() to get an array of unique values,\n",
    "# and then accessing the first element of the .shape attribute, which represents the number of unique subjects\n",
    "\n",
    "corr_df.subj.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89743492-d9fe-4730-8924-26767a32e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the correlation dataframe to a csv file for later analysis\n",
    "# Update filename based on epoch duration\n",
    "\n",
    "corr_df.to_csv(\"preprocessed_interpolate_bandpass_.1_20/merged_corr_df_1s_epoch.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
